<<<<<<< HEAD
# Production-Grade RAG Agent using NVIDIA LLMs

A **production-style Retrieval-Augmented Generation (RAG) system** built from scratch using NVIDIAâ€™s Integrate API.  
This project demonstrates **real-world RAG engineering practices**: document ingestion, robust chunking, embedding caching, semantic retrieval, citation-grounded answering, and automatic evaluation.

>  Goal: build a RAG system that is **explainable, reliable, and measurable** â€” not a demo script.

---

##  Key Features

-  Ingests **TXT / MD / PDF** documents
-  PDF-safe chunking with overlap (no broken paragraphs)
-  NVIDIA embeddings with **on-disk caching**
-  Cosine similarity retrieval (local, fast)
-  Strict citation-only answering (no hallucinations)
-  Confidence thresholds + clarification handling
-  Automatic RAG evaluation metrics (JSONL)
-  Modular, production-ready architecture

---

##  Project Structure

```
nvidia_rag/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agent.py        # Interactive production RAG agent
â”‚   â”œâ”€â”€ cache.py        # Chunk fingerprinting + embedding cache
â”‚   â”œâ”€â”€ chunk.py        # Robust chunking logic (PDF-safe)
â”‚   â”œâ”€â”€ config.py       # Central configuration (models, paths, thresholds)
â”‚   â”œâ”€â”€ embed.py        # NVIDIA embedding wrapper (batched)
â”‚   â”œâ”€â”€ eval.py         # RAG evaluation + metrics logging
â”‚   â”œâ”€â”€ ingest.py       # Document ingestion (TXT / MD / PDF)
â”‚   â”œâ”€â”€ llm.py          # NVIDIA chat wrapper (retry-safe)
â”‚   â”œâ”€â”€ prompt.py       # Strict citation prompt
â”‚   â””â”€â”€ retrieve.py     # Cosine similarity retrieval
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ public_docs/    # Place source documents here
â”‚
â”œâ”€â”€ cache/              # Auto-generated chunks + embeddings
â”œâ”€â”€ metrics/            # Auto-generated evaluation logs
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_and_chunk.py
â”‚   â”œâ”€â”€ build_cache.py
â”‚   â”œâ”€â”€ run_agent.py
â”‚   â””â”€â”€ show_metrics.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

##  Setup

### 1ï¸âƒ£ Create & activate virtual environment

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

### 2ï¸âƒ£ Install dependencies

```powershell
python -m pip install -r requirements.txt
```

### 3ï¸âƒ£ Set NVIDIA API key

```powershell
$env:NVIDIA_API_KEY="nvapi-your-key-here"
``` 
> Use `.env.example` as a template.

---

## â–¶ï¸ Running the System

### Step 1 â€” Add documents
Place your files in:

```
data/public_docs/
```

Supported formats:
- `.txt`
- `.md`
- `.pdf`

---

### Step 2 â€” Ingest & chunk documents

```powershell
python -m nvidia_rag.scripts.ingest_and_chunk
```

Example output:
```
Loaded documents: 4
Created chunks: 121
Saved: cache/chunks.json
```

---

### Step 3 â€” Build embedding cache (one-time per dataset)

```powershell
python -m nvidia_rag.scripts.build_cache
```

Example output:
```
Cache missing â€” embedding chunks once...
Chunk vectors shape: (121, 1024)
```

> Embeddings are cached using a fingerprint of content + model.  
> Cache is reused automatically unless documents change.

---

### Step 4 â€” Run the RAG agent

```powershell
python -m nvidia_rag.scripts.run_agent
```

Example interaction:

```
You: whats the procedure of Conversion from UG to GmbH?

Top sources:
- [Requirements_Company_founding#9] score=0.597
- [Requirements_Company_founding#2] score=0.588

Assistant:
A UG can convert to a GmbH once it accumulates â‚¬25,000 in share capital reserves [Requirements_Company_founding#9].
Additionally, the company must retain 25% of its annual profits until this threshold is reached [Requirements_Company_founding#2].
```

---

##  Evaluation & Metrics

Each query is logged automatically to:

```
metrics/rag_metrics.jsonl
```

Example record:

```json
{
  "query": "what is RAG?",
  "top_score": 0.39,
  "num_chunks": 3,
  "threshold_used": 0.25,
  "vague_query": false,
  "has_citation": true,
  "answer_len": 118,
  "sources": ["doc1#0"],
  "timestamp": "2025-12-22T13:01:45"
}
```

View metrics summary:

```powershell
python -m nvidia_rag.scripts.show_metrics
```

---

## ðŸ§  System Flow

```
User Query
   â†“
Query Embedding (NVIDIA)
   â†“
Cosine Similarity
   â†“
Top-K Relevant Chunks
   â†“
Citation-Strict Prompt
   â†“
NVIDIA LLM
   â†“
Answer + Evaluation Metrics
```

---

## ðŸ›¡ï¸ Production Safeguards

- Content-based cache fingerprinting
- Chunk size enforcement (token-safe)
- Batched embedding requests
- Automatic retry on empty LLM responses
- Clarification prompts for low-confidence queries
- Strict citation enforcement (no hallucinations)

---

## ðŸ§‘â€ðŸ’» CLI Usage

```bash
rag ingest        # Ingest & chunk documents
rag build         # Build embedding cache
rag ask "..."     # Ask a single question
rag run           # Interactive agent


## ðŸ§ª Example Questions

- `what is RAG?`
- `who is Abdul Rahman?`
- `what are the requirements for a GmbH?`
- `does the supervisory board required?`
- `who is he?` â†’ agent requests clarification

---

## ðŸ“Œ Design Philosophy

This project intentionally avoids vector databases to:
- demonstrate **core RAG mechanics**
- make retrieval transparent and debuggable
- focus on correctness before scale

The architecture can be extended to FAISS, Milvus, or Pinecone with minimal changes.

---

## ðŸ License

MIT

---

## ðŸ™Œ Acknowledgment

Built as a hands-on production learning project using NVIDIAâ€™s LLM and embedding APIs.
=======
# production-rag-agent
A production-grade Retrieval-Augmented Generation (RAG) system using NVIDIA LLMs, featuring document ingestion, smart chunking, embedding caching, FastAPI backend, CLI tooling, Docker deployment, and automated evaluation metrics.
>>>>>>> 4d9f5603ae19c43863b41ec473d95d00dc7d1eff
